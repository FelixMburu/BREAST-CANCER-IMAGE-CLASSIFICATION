# -*- coding: utf-8 -*-
"""STREAMLIT DEPLOYMENT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/134O806vspNxSCfsimVPOixFu2_LQi0p2
"""

#Steamlit
import streamlit as st
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image

# Define your model architecture - use the same architecture as the one you trained and saved
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        # Define layers here - make sure these match the saved model
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.flattened_size = self._get_flattened_size()
        self.fc1 = nn.Linear(self.flattened_size, 128)
        self.fc2 = nn.Linear(128, 2)

    def _get_flattened_size(self):
        dummy_input = torch.zeros(1, 3, 50, 50)
        x = self.pool(F.relu(self.conv1(dummy_input)))
        x = self.pool(F.relu(self.conv2(x)))
        return x.numel()

    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        x = x.view(-1, self.flattened_size)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Load the model
model = MyModel()
model.load_state_dict(torch.load('best_model.pth')) # Now the model architecture should match
model.eval()

# Define a transform
transform = transforms.Compose([
    transforms.Resize((50, 50)), # Adjust the size if needed to match your training data
    transforms.ToTensor()
])

# Streamlit app
st.title('Model Deployment with Streamlit')

uploaded_file = st.file_uploader("Choose an image...", type="jpg")

if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image.', use_column_width=True)
    st.write("")
    st.write("Classifying...")

    image = transform(image)
    image = image.unsqueeze(0)  # Add batch dimension
    output = model(image)
    _, predicted = torch.max(output, 1)
    st.write(f'Predicted class: {predicted.item()}')
